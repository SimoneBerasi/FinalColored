import keras.metrics

from DataLoader import *
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image_dataset_from_directory
import matplotlib.pyplot as plt
from tensorflow.keras.applications.vgg16 import preprocess_input
from sklearn.feature_extraction.image import extract_patches_2d
from NNModels import Model_noise_skip
import os
from CustomGenerator import  CustomSequence
from PIL import Image
from random import sample
from ColorUtils import *
from skimage.color import rgb2lab, lab2rgb
#https://github.com/jkjung-avt/keras-cats-dogs-tutorial
from Training import color_cwssim_loss

train_directory = '/home/simo/Desktop/Thesis Projects/AnomalyDetectionBionda/Dataset/MVTec_Data/wood/train/good'
#train_directory = '/home/simo/Desktop/Thesis Projects/AnomalyDetectionBionda/try/try'
patches_per_image = 1

#crop and apply prepare_image-cssim
def random_crop(img, random_crop_size):
    # Note: image_data_format is 'channel_last'
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    img = img[y:(y+dy), x:(x+dx), :]
    plt.imshow(img/255.)
    plt.show()
    img = img.astype(np.uint8)
    img = prepare_image_colorssim(img)
    return img


def crop_generator(batches, crop_length):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    """
    while True:
        batch_x, batch_y = next(batches)
        batch_crops = np.zeros((batch_x.shape[0]*patches_per_image, crop_length, crop_length, 3))
        for i in range(batch_x.shape[0]):
            for j in range(patches_per_image):
                batch_crops[i*patches_per_image +j] = random_crop(batch_x[i], (crop_length, crop_length))
        yield (batch_crops, batch_y)

def preprocessing_function(x):

    x = x.astype(np.uint8)

    x = prepare_image_colorssim(x)

    return x




if __name__ == '__main__':
    tf.keras.backend.set_floatx('float64')


    gpus = tf.config.list_physical_devices('GPU')
    tf.config.experimental.set_memory_growth(gpus[0], True)

    filenames = os.listdir(train_directory)

    path_filnames = []
    for name in filenames:
        path_filnames.append(train_directory + '/' + name)



    training_batch_generator = CustomSequence(path_filnames, 247)
    train_dataset = tf.data.Dataset.from_generator(lambda: training_batch_generator, output_types=(tf.float64, tf.float64))
    train_dataset = train_dataset.repeat(100)



    model = Model_noise_skip(input_shape=(128,128,3))
    model.compile(optimizer='adam', loss=color_cwssim_loss)
    model.fit(train_dataset, batch_size=5,  steps_per_epoch=67, epochs = 100, use_multiprocessing=True, workers=16)

    exit(0)

    model = Model_noise_skip(input_shape=(128,128,3))
    model.compile(optimizer='adam', loss=color_cwssim_loss)


    model.fit(training_batch_generator,
                                          steps_per_epoch=2,
                                          epochs=10,
              multiprocessing = True,
)

    exit(0)

    batch_size = 250

    train_datagen = ImageDataGenerator()
    train_batches = train_datagen.flow_from_directory(train_directory, class_mode='input', batch_size=1, target_size= (128, 128))



    train_patches = crop_generator(train_batches, 128)



    train_dataset = tf.data.Dataset.from_generator(lambda: train_patches, output_types=(tf.float64, tf.float64))


    train_dataset = train_dataset.repeat()

    for i in train_dataset:
        pass

    exit(0)
    for i in train_dataset:
        plt.imshow(i[0][0])
        plt.show()




    j=0
    for images, labels in train_patches:
        j=j+1


        for i in range(batch_size):
            print(i)
            a = images[i]

        break
